{
  Preconditions.checkNotNull(this.destination);
  Preconditions.checkArgument(!Strings.isNullOrEmpty(this.writerId));
  Preconditions.checkNotNull(this.dataConverter);
  Preconditions.checkNotNull(this.schemaConverter);
  Preconditions.checkNotNull(this.sourceSchema);
  Preconditions.checkNotNull(this.schemaType);
  Preconditions.checkArgument(this.format == WriterOutputFormat.AVRO);
  Schema schema;
  try {
    schema=this.schemaConverter.convert(this.sourceSchema);
  }
 catch (  SchemaConversionException e) {
    throw new IOException("Failed to convert the source schema: " + this.sourceSchema);
  }
  if (!this.schemaType.getSchemaValidator().validate(schema)) {
    throw new IOException(String.format("Schema of type %s could not be validated",this.schemaType.name()));
  }
switch (this.destination.getType()) {
case HDFS:
    Properties properties=this.destination.getProperties();
  String uri=properties.getProperty(ConfigurationKeys.FILE_SYSTEM_URI_KEY);
String stagingDir=properties.getProperty(ConfigurationKeys.STAGING_DIR_KEY,ConfigurationKeys.DEFAULT_STAGING_DIR);
String outputDir=properties.getProperty(ConfigurationKeys.OUTPUT_DIR_KEY,ConfigurationKeys.DEFAULT_OUTPUT_DIR);
String fileName=properties.getProperty(ConfigurationKeys.FILE_NAME_KEY) + "." + this.writerId;
int bufferSize=Integer.parseInt(properties.getProperty(ConfigurationKeys.BUFFER_SIZE_KEY,ConfigurationKeys.DEFAULT_BUFFER_SIZE));
return new AvroHdfsDataWriter<DI>(URI.create(uri),stagingDir,outputDir,fileName,bufferSize,this.dataConverter,schema);
case KAFKA:
return new KafkaDataWriter<DI>();
default :
throw new RuntimeException("Unknown destination type: " + this.destination.getType());
}
}
