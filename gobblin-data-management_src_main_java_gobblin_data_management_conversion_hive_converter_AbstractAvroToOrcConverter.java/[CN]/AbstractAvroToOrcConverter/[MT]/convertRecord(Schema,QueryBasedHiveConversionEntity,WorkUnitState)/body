{
  Preconditions.checkNotNull(outputAvroSchema,"Avro schema must not be null");
  Preconditions.checkNotNull(conversionEntity,"Conversion entity must not be null");
  Preconditions.checkNotNull(workUnit,"Workunit state must not be null");
  Preconditions.checkNotNull(conversionEntity.getHiveTable(),"Hive table within conversion entity must not be null");
  EventWorkunitUtils.setBeginDDLBuildTimeMetadata(workUnit,System.currentTimeMillis());
  this.hiveDataset=conversionEntity.getConvertibleHiveDataset();
  if (!hasConversionConfig()) {
    return new SingleRecordIterable<>(conversionEntity);
  }
  String avroTableName=conversionEntity.getHiveTable().getTableName();
  String orcTableName=getConversionConfig().getDestinationTableName();
  String orcStagingTableName=getOrcStagingTableName(getConversionConfig().getDestinationStagingTableName());
  String orcTableDatabase=getConversionConfig().getDestinationDbName();
  String orcDataLocation=getOrcDataLocation();
  String orcStagingDataLocation=getOrcStagingDataLocation(orcStagingTableName);
  boolean isEvolutionEnabled=getConversionConfig().isEvolutionEnabled();
  Pair<Optional<Table>,Optional<List<Partition>>> destinationMeta=getDestinationTableMeta(orcTableDatabase,orcTableName,workUnit);
  Optional<Table> destinationTableMeta=destinationMeta.getLeft();
  Optional<List<String>> clusterBy=getConversionConfig().getClusterBy().isEmpty() ? Optional.<List<String>>absent() : Optional.of(getConversionConfig().getClusterBy());
  Optional<Integer> numBuckets=getConversionConfig().getNumBuckets();
  Optional<Integer> rowLimit=getConversionConfig().getRowLimit();
  Properties tableProperties=getConversionConfig().getDestinationTableProperties();
  List<String> sourceDataPathIdentifier=getConversionConfig().getSourceDataPathIdentifier();
  Map<String,String> partitionsDDLInfo=Maps.newHashMap();
  Map<String,String> partitionsDMLInfo=Maps.newHashMap();
  populatePartitionInfo(conversionEntity,partitionsDDLInfo,partitionsDMLInfo);
  try {
    FsPermission sourceDataPermission=this.fs.getFileStatus(conversionEntity.getHiveTable().getDataLocation()).getPermission();
    if (!this.fs.mkdirs(new Path(getConversionConfig().getDestinationDataPath()),sourceDataPermission)) {
      throw new RuntimeException(String.format("Failed to create path %s with permissions %s",new Path(getConversionConfig().getDestinationDataPath()),sourceDataPermission));
    }
 else {
      this.fs.setPermission(new Path(getConversionConfig().getDestinationDataPath()),sourceDataPermission);
      log.info(String.format("Created %s with permissions %s",new Path(getConversionConfig().getDestinationDataPath()),sourceDataPermission));
    }
  }
 catch (  IOException e) {
    Throwables.propagate(e);
  }
  for (  Map.Entry<Object,Object> entry : getConversionConfig().getHiveRuntimeProperties().entrySet()) {
    conversionEntity.getQueries().add(String.format("SET %s=%s",entry.getKey(),entry.getValue()));
  }
  conversionEntity.getQueries().add(String.format("SET %s=%s",GOBBLIN_DATASET_URN_KEY,conversionEntity.getHiveTable().getCompleteName()));
  if (conversionEntity.getHivePartition().isPresent()) {
    conversionEntity.getQueries().add(String.format("SET %s=%s",GOBBLIN_PARTITION_NAME_KEY,conversionEntity.getHivePartition().get().getCompleteName()));
  }
  conversionEntity.getQueries().add(String.format("SET %s=%s",GOBBLIN_WORKUNIT_CREATE_TIME_KEY,workUnit.getWorkunit().getProp(SlaEventKeys.ORIGIN_TS_IN_MILLI_SECS_KEY)));
  Map<String,String> hiveColumns=new HashMap<>();
  String createStagingTableDDL=HiveAvroORCQueryGenerator.generateCreateTableDDL(outputAvroSchema,orcStagingTableName,orcStagingDataLocation,Optional.of(orcTableDatabase),Optional.of(partitionsDDLInfo),clusterBy,Optional.<Map<String,HiveAvroORCQueryGenerator.COLUMN_SORT_ORDER>>absent(),numBuckets,Optional.<String>absent(),Optional.<String>absent(),Optional.<String>absent(),tableProperties,isEvolutionEnabled,destinationTableMeta,hiveColumns);
  conversionEntity.getQueries().add(createStagingTableDDL);
  log.info("Create staging table DDL: " + createStagingTableDDL);
  String orcStagingDataPartitionDirName=getOrcStagingDataPartitionDirName(conversionEntity,sourceDataPathIdentifier);
  String orcStagingDataPartitionLocation=orcStagingDataLocation + Path.SEPARATOR + orcStagingDataPartitionDirName;
  if (partitionsDMLInfo.size() > 0) {
    List<String> createStagingPartitionDDL=HiveAvroORCQueryGenerator.generateCreatePartitionDDL(orcTableDatabase,orcStagingTableName,orcStagingDataPartitionLocation,partitionsDMLInfo);
    conversionEntity.getQueries().addAll(createStagingPartitionDDL);
    log.info("Create staging partition DDL: " + createStagingPartitionDDL);
  }
  String insertInORCStagingTableDML=HiveAvroORCQueryGenerator.generateTableMappingDML(conversionEntity.getHiveTable().getAvroSchema(),outputAvroSchema,avroTableName,orcStagingTableName,Optional.of(conversionEntity.getHiveTable().getDbName()),Optional.of(orcTableDatabase),Optional.of(partitionsDMLInfo),Optional.<Boolean>absent(),Optional.<Boolean>absent(),isEvolutionEnabled,destinationTableMeta,rowLimit);
  conversionEntity.getQueries().add(insertInORCStagingTableDML);
  log.info("Conversion staging DML: " + insertInORCStagingTableDML);
  QueryBasedHivePublishEntity publishEntity=new QueryBasedHivePublishEntity();
  List<String> publishQueries=publishEntity.getPublishQueries();
  Map<String,String> publishDirectories=publishEntity.getPublishDirectories();
  List<String> cleanupQueries=publishEntity.getCleanupQueries();
  List<String> cleanupDirectories=publishEntity.getCleanupDirectories();
  if (!destinationTableMeta.isPresent()) {
    String createTargetTableDDL=HiveAvroORCQueryGenerator.generateCreateTableDDL(outputAvroSchema,orcTableName,orcDataLocation,Optional.of(orcTableDatabase),Optional.of(partitionsDDLInfo),clusterBy,Optional.<Map<String,HiveAvroORCQueryGenerator.COLUMN_SORT_ORDER>>absent(),numBuckets,Optional.<String>absent(),Optional.<String>absent(),Optional.<String>absent(),tableProperties,isEvolutionEnabled,destinationTableMeta,new HashMap<String,String>());
    publishQueries.add(createTargetTableDDL);
    log.info("Create final table DDL: " + createTargetTableDDL);
  }
  List<String> evolutionDDLs=HiveAvroORCQueryGenerator.generateEvolutionDDL(orcStagingTableName,orcTableName,Optional.of(orcTableDatabase),Optional.of(orcTableDatabase),outputAvroSchema,isEvolutionEnabled,hiveColumns,destinationTableMeta);
  log.info("Evolve final table DDLs: " + evolutionDDLs);
  EventWorkunitUtils.setEvolutionMetadata(workUnit,evolutionDDLs);
  publishQueries.addAll(evolutionDDLs);
  if (partitionsDDLInfo.size() == 0) {
    log.info("Snapshot directory to move: " + orcStagingDataLocation + " to: "+ orcDataLocation);
    publishDirectories.put(orcStagingDataLocation,orcDataLocation);
    String dropStagingTableDDL=HiveAvroORCQueryGenerator.generateDropTableDDL(orcTableDatabase,orcStagingTableName);
    log.info("Drop staging table DDL: " + dropStagingTableDDL);
    cleanupQueries.add(dropStagingTableDDL);
    log.info("Staging table directory to delete: " + orcStagingDataLocation);
    cleanupDirectories.add(orcStagingDataLocation);
  }
 else {
    List<String> dropPartitionsDDL=HiveAvroORCQueryGenerator.generateDropPartitionsDDL(orcTableDatabase,orcTableName,partitionsDMLInfo);
    log.info("Drop partitions if exist in final table: " + dropPartitionsDDL);
    publishQueries.addAll(dropPartitionsDDL);
    String orcFinalDataPartitionLocation=orcDataLocation + Path.SEPARATOR + orcStagingDataPartitionDirName;
    log.info("Partition directory to move: " + orcStagingDataPartitionLocation + " to: "+ orcFinalDataPartitionLocation);
    publishDirectories.put(orcStagingDataPartitionLocation,orcFinalDataPartitionLocation);
    String orcDataPartitionLocation=orcDataLocation + Path.SEPARATOR + orcStagingDataPartitionDirName;
    List<String> createFinalPartitionDDL=HiveAvroORCQueryGenerator.generateCreatePartitionDDL(orcTableDatabase,orcTableName,orcDataPartitionLocation,partitionsDMLInfo);
    log.info("Create final partition DDL: " + createFinalPartitionDDL);
    publishQueries.addAll(createFinalPartitionDDL);
    String dropStagingTableDDL=HiveAvroORCQueryGenerator.generateDropTableDDL(orcTableDatabase,orcStagingTableName);
    log.info("Drop staging table DDL: " + dropStagingTableDDL);
    cleanupQueries.add(dropStagingTableDDL);
    log.info("Staging table directory to delete: " + orcStagingDataLocation);
    cleanupDirectories.add(orcStagingDataLocation);
  }
  publishQueries.addAll(HiveAvroORCQueryGenerator.generateDropPartitionsDDL(orcTableDatabase,orcTableName,getDropPartitionsDDLInfo(conversionEntity)));
  HiveAvroORCQueryGenerator.serializePublishCommands(workUnit,publishEntity);
  log.info("Publish partition entity: " + publishEntity);
  log.debug("Conversion Query " + conversionEntity.getQueries());
  EventWorkunitUtils.setEndDDLBuildTimeMetadata(workUnit,System.currentTimeMillis());
  return new SingleRecordIterable<>(conversionEntity);
}
