{
  this.metricContext=Instrumented.getMetricContext(state,HiveSource.class);
  this.eventSubmitter=new EventSubmitter.Builder(this.metricContext,EventConstants.CONVERSION_NAMESPACE).build();
  List<WorkUnit> workunits=Lists.newArrayList();
  try {
    EventSubmitter.submit(Optional.of(this.eventSubmitter),EventConstants.SETUP_EVENT);
    HiveSourceWatermarker watermaker=new TableLevelWatermarker(state);
    HiveUnitUpdateProviderFactory updateProviderFactory=GobblinConstructorUtils.invokeConstructor(HiveUnitUpdateProviderFactory.class,state.getProp(OPTIONAL_HIVE_UNIT_UPDATE_PROVIDER_FACTORY_CLASS_KEY,DEFAULT_HIVE_UNIT_UPDATE_PROVIDER_FACTORY_CLASS));
    HiveUnitUpdateProvider updateProvider=updateProviderFactory.create(state);
    IterableDatasetFinder<HiveDataset> datasetFinder=new HiveDatasetFinder(getSourceFs(),state.getProperties(),this.eventSubmitter);
    AvroSchemaManager avroSchemaManager=new AvroSchemaManager(getSourceFs(),state);
    EventSubmitter.submit(Optional.of(this.eventSubmitter),EventConstants.FIND_HIVE_TABLES_EVENT);
    Iterator<HiveDataset> iterator=datasetFinder.getDatasetsIterator();
    while (iterator.hasNext()) {
      HiveDataset hiveDataset=iterator.next();
      LongWatermark expectedDatasetHighWatermark=new LongWatermark(new DateTime().getMillis());
      log.debug(String.format("Processing dataset: %s",hiveDataset));
      if (HiveUtils.isPartitioned(hiveDataset.getTable())) {
        List<Partition> sourcePartitions=HiveUtils.getPartitions(hiveDataset.getClientPool().getClient().get(),hiveDataset.getTable(),Optional.<String>absent());
        for (        Partition sourcePartition : sourcePartitions) {
          LongWatermark lowWatermark=watermaker.getPreviousHighWatermark(sourcePartition);
          long updateTime=updateProvider.getUpdateTime(sourcePartition);
          if (Long.compare(updateTime,lowWatermark.getValue()) > 0) {
            log.debug(String.format("Processing partition: %s",sourcePartition));
            WorkUnit workUnit=WorkUnit.createEmpty();
            workUnit.setProp(ConfigurationKeys.DATASET_URN_KEY,hiveDataset.getTable().getCompleteName());
            HiveSourceUtils.serializeTable(workUnit,hiveDataset.getTable(),avroSchemaManager);
            HiveSourceUtils.serializePartition(workUnit,sourcePartition,avroSchemaManager);
            workUnit.setWatermarkInterval(new WatermarkInterval(lowWatermark,expectedDatasetHighWatermark));
            HiveSourceUtils.setPartitionSlaEventMetadata(workUnit,hiveDataset.getTable(),sourcePartition,updateTime,lowWatermark.getValue());
            workunits.add(workUnit);
            log.debug(String.format("Workunit added for partition: %s",workUnit));
          }
 else {
            log.info(String.format("Not creating workunit for partition %s as updateTime %s is lesser than low watermark %s",sourcePartition.getCompleteName(),updateTime,lowWatermark.getValue()));
          }
        }
      }
 else {
        long updateTime=updateProvider.getUpdateTime(hiveDataset.getTable());
        LongWatermark lowWatermark=watermaker.getPreviousHighWatermark(hiveDataset.getTable());
        if (Long.compare(updateTime,lowWatermark.getValue()) > 0) {
          log.debug(String.format("Processing table: %s",hiveDataset.getTable()));
          WorkUnit workUnit=WorkUnit.createEmpty();
          workUnit.setProp(ConfigurationKeys.DATASET_URN_KEY,hiveDataset.getTable().getCompleteName());
          HiveSourceUtils.serializeTable(workUnit,hiveDataset.getTable(),avroSchemaManager);
          workUnit.setWatermarkInterval(new WatermarkInterval(lowWatermark,expectedDatasetHighWatermark));
          HiveSourceUtils.setTableSlaEventMetadata(workUnit,hiveDataset.getTable(),updateTime,lowWatermark.getValue());
          workunits.add(workUnit);
          log.debug(String.format("Workunit added for table: %s",workUnit));
        }
 else {
          log.info(String.format("Not creating workunit for table %s as updateTime %s is lesser than low watermark %s",hiveDataset.getTable().getCompleteName(),updateTime,lowWatermark.getValue()));
        }
      }
    }
  }
 catch (  IOException e) {
    throw new RuntimeException(e);
  }
  return workunits;
}
