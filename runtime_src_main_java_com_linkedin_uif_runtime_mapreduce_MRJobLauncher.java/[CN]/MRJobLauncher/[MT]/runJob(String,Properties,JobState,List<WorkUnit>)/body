{
  for (  String name : jobProps.stringPropertyNames()) {
    this.conf.set(name,jobProps.getProperty(name));
  }
  Path mrJobDir=new Path(jobProps.getProperty(ConfigurationKeys.MR_JOB_ROOT_DIR_KEY),jobName);
  if (this.fs.exists(mrJobDir)) {
    LOG.warn("Job working directory already exists for job " + jobName);
    this.fs.delete(mrJobDir,true);
  }
  Path jarFileDir=new Path(mrJobDir,"_jars");
  if (jobProps.containsKey(ConfigurationKeys.FRAMEWORK_JAR_FILES_KEY)) {
    addJars(jarFileDir,jobProps.getProperty(ConfigurationKeys.FRAMEWORK_JAR_FILES_KEY));
  }
  if (jobProps.containsKey(ConfigurationKeys.JOB_JAR_FILES_KEY)) {
    addJars(jarFileDir,jobProps.getProperty(ConfigurationKeys.JOB_JAR_FILES_KEY));
  }
  if (jobProps.containsKey(ConfigurationKeys.JOB_LOCAL_FILES_KEY)) {
    addLocalFiles(new Path(mrJobDir,"_files"),jobProps.getProperty(ConfigurationKeys.JOB_LOCAL_FILES_KEY));
  }
  if (jobProps.containsKey(ConfigurationKeys.JOB_HDFS_FILES_KEY)) {
    addHDFSFiles(jobProps.getProperty(ConfigurationKeys.JOB_HDFS_FILES_KEY));
  }
  this.conf.set("mapred.max.map.failures.percent","100");
  this.conf.set("mapreduce.map.failures.maxpercent","100");
  this.job=Job.getInstance(this.conf,JOB_NAME_PREFIX + jobName);
  this.job.setJarByClass(MRJobLauncher.class);
  this.job.setMapperClass(TaskRunner.class);
  this.job.setNumReduceTasks(0);
  this.job.setInputFormatClass(NLineInputFormat.class);
  this.job.setOutputFormatClass(NullOutputFormat.class);
  this.job.setMapOutputKeyClass(NullWritable.class);
  this.job.setMapOutputValueClass(NullWritable.class);
  this.job.setSpeculativeExecution(false);
  Path jobInputPath=new Path(mrJobDir,"input");
  Path jobInputFile=prepareJobInput(jobProps.getProperty(ConfigurationKeys.JOB_ID_KEY),jobInputPath,workUnits);
  NLineInputFormat.addInputPath(this.job,jobInputFile);
  Path jobOutputPath=new Path(mrJobDir,"output");
  SequenceFileOutputFormat.setOutputPath(this.job,jobOutputPath);
  if (jobProps.containsKey(ConfigurationKeys.MR_JOB_MAX_MAPPERS_KEY)) {
    int maxMappers=Integer.parseInt(jobProps.getProperty(ConfigurationKeys.MR_JOB_MAX_MAPPERS_KEY));
    if (workUnits.size() > maxMappers) {
      int numTasksPerMapper=workUnits.size() % maxMappers == 0 ? workUnits.size() / maxMappers : workUnits.size() / maxMappers + 1;
      NLineInputFormat.setNumLinesPerSplit(this.job,numTasksPerMapper);
    }
  }
  try {
    LOG.info("Launching Hadoop MR job " + this.job.getJobName());
    this.job.submit();
    if (!jobState.contains(ConfigurationKeys.JOB_TRACKING_URL_KEY)) {
      jobState.setProp(ConfigurationKeys.JOB_TRACKING_URL_KEY,job.getTrackingURL());
    }
    this.job.waitForCompletion(true);
    if (this.isCancelled) {
      jobState.setState(JobState.RunningState.CANCELLED);
      return;
    }
    jobState.setState(this.job.isSuccessful() ? JobState.RunningState.SUCCESSFUL : JobState.RunningState.FAILED);
    jobState.addTaskStates(collectOutput(new Path(jobOutputPath,jobProps.getProperty(ConfigurationKeys.JOB_ID_KEY))));
    countersToMetrics(this.job.getCounters(),JobMetrics.get(jobName,jobProps.getProperty(ConfigurationKeys.JOB_ID_KEY)));
  }
  finally {
    try {
      if (this.fs.exists(mrJobDir)) {
        this.fs.delete(mrJobDir,true);
      }
    }
 catch (    IOException ioe) {
      LOG.error("Failed to cleanup job working directory for job " + this.job.getJobID());
    }
  }
}
