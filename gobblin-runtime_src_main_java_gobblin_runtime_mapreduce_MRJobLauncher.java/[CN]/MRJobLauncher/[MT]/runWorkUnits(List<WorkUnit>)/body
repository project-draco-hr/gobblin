{
  String jobName=this.jobContext.getJobName();
  JobState jobState=this.jobContext.getJobState();
  try {
    TimingEvent stagingDataCleanTimer=this.eventSubmitter.getTimingEvent(TimingEventNames.RunJobTimings.MR_STAGING_DATA_CLEAN);
    for (    WorkUnit workUnit : JobLauncherUtils.flattenWorkUnits(workUnits)) {
      WorkUnit fatWorkUnit=WorkUnit.copyOf(workUnit);
      fatWorkUnit.addAllIfNotExist(jobState);
      JobLauncherUtils.cleanStagingData(fatWorkUnit,LOG);
    }
    stagingDataCleanTimer.stop();
    Path jobOutputPath=prepareHadoopJob(workUnits);
    LOG.info("Launching Hadoop MR job " + this.job.getJobName());
    this.job.submit();
    this.hadoopJobSubmitted=true;
    if (!jobState.contains(ConfigurationKeys.JOB_TRACKING_URL_KEY)) {
      jobState.setProp(ConfigurationKeys.JOB_TRACKING_URL_KEY,this.job.getTrackingURL());
    }
    TimingEvent mrJobRunTimer=this.eventSubmitter.getTimingEvent(TimingEventNames.RunJobTimings.MR_JOB_RUN);
    LOG.info(String.format("Waiting for Hadoop MR job %s to complete",this.job.getJobID()));
    this.job.waitForCompletion(true);
    mrJobRunTimer.stop();
    if (this.cancellationRequested) {
synchronized (this.cancellationExecution) {
        if (this.cancellationExecuted) {
          return;
        }
      }
    }
    jobState.setState(this.job.isSuccessful() ? JobState.RunningState.SUCCESSFUL : JobState.RunningState.FAILED);
    List<TaskState> outputTaskStates=collectOutputTaskStates(new Path(jobOutputPath,jobState.getJobId()));
    if (outputTaskStates.size() < jobState.getTasks()) {
      LOG.error(String.format("Collected %d task states while expecting %d task states",outputTaskStates.size(),jobState.getTasks()));
      jobState.setState(JobState.RunningState.FAILED);
    }
    jobState.addTaskStates(outputTaskStates);
    countersToMetrics(Optional.fromNullable(this.job.getCounters()),JobMetrics.get(jobName,this.jobProps.getProperty(ConfigurationKeys.JOB_ID_KEY)));
  }
  finally {
    cleanUpWorkingDirectory();
  }
}
