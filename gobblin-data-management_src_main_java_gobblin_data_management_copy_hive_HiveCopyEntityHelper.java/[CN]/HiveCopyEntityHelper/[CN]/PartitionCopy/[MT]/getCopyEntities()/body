{
  try (Closer closer=Closer.create()){
    log.info("Getting copy entities for partition " + this.partition.getCompleteName());
    MultiTimingEvent multiTimer=closer.register(new MultiTimingEvent(this.eventSubmitter,"PartitionCopy",true));
    int stepPriority=0;
    String fileSet=gson.toJson(this.partition.getValues());
    List<CopyEntity> copyEntities=Lists.newArrayList();
    stepPriority=addSharedSteps(copyEntities,fileSet,stepPriority);
    multiTimer.nextStage(Stages.COMPUTE_TARGETS);
    Path targetPath=getTargetLocation(dataset.fs,targetFs,this.partition.getDataLocation(),Optional.of(this.partition));
    Partition targetPartition=getTargetPartition(this.partition,targetPath);
    multiTimer.nextStage(Stages.EXISTING_PARTITION);
    if (this.existingTargetPartition.isPresent()) {
      targetPartitions.remove(partition.getValues());
      try {
        checkPartitionCompatibility(targetPartition,existingTargetPartition.get());
      }
 catch (      IOException ioe) {
        if (existingEntityPolicy != ExistingEntityPolicy.REPLACE_PARTITIONS) {
          log.error("Source and target partitions are not compatible. Aborting copy of partition " + this.partition,ioe);
          return Lists.newArrayList();
        }
        log.warn("Source and target partitions are not compatible. Will override target partition.",ioe);
        stepPriority=addDeregisterSteps(copyEntities,fileSet,stepPriority,targetTable,existingTargetPartition.get());
        existingTargetPartition=Optional.absent();
      }
    }
    multiTimer.nextStage(Stages.PARTITION_SKIP_PREDICATE);
    if (fastPartitionSkip.isPresent() && fastPartitionSkip.get().apply(this)) {
      log.info(String.format("Skipping copy of partition %s due to fast partition skip predicate.",this.partition.getCompleteName()));
      return Lists.newArrayList();
    }
    HiveSpec partitionHiveSpec=new SimpleHiveSpec.Builder<>(targetPath).withTable(HiveMetaStoreUtils.getHiveTable(targetTable.getTTable())).withPartition(Optional.of(HiveMetaStoreUtils.getHivePartition(targetPartition.getTPartition()))).build();
    HiveRegisterStep register=new HiveRegisterStep(targetURI,partitionHiveSpec,hiveRegProps);
    copyEntities.add(new PostPublishStep(fileSet,Maps.<String,Object>newHashMap(),register,stepPriority++));
    multiTimer.nextStage(Stages.CREATE_LOCATIONS);
    HiveLocationDescriptor sourceLocation=HiveLocationDescriptor.forPartition(this.partition,dataset.fs,properties);
    HiveLocationDescriptor desiredTargetLocation=HiveLocationDescriptor.forPartition(targetPartition,targetFs,properties);
    Optional<HiveLocationDescriptor> existingTargetLocation=existingTargetPartition.isPresent() ? Optional.of(HiveLocationDescriptor.forPartition(existingTargetPartition.get(),targetFs,properties)) : Optional.<HiveLocationDescriptor>absent();
    multiTimer.nextStage(Stages.FULL_PATH_DIFF);
    DiffPathSet diffPathSet=fullPathDiff(sourceLocation,desiredTargetLocation,existingTargetLocation,Optional.<Partition>absent(),multiTimer,HiveCopyEntityHelper.this);
    multiTimer.nextStage(Stages.CREATE_DELETE_UNITS);
    if (diffPathSet.pathsToDelete.size() > 0) {
      DeleteFileCommitStep deleteStep=DeleteFileCommitStep.fromPaths(targetFs,diffPathSet.pathsToDelete,dataset.properties);
      copyEntities.add(new PrePublishStep(fileSet,Maps.<String,Object>newHashMap(),deleteStep,stepPriority++));
    }
    multiTimer.nextStage(Stages.CREATE_COPY_UNITS);
    for (    CopyableFile.Builder builder : getCopyableFilesFromPaths(diffPathSet.filesToCopy,configuration,Optional.of(partition))) {
      copyEntities.add(builder.fileSet(fileSet).checksum(new byte[0]).build());
    }
    return copyEntities;
  }
 }
